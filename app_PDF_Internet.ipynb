{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import io\n",
    "import requests\n",
    "from langchain_community.llms import LlamaCpp\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "from langchain.embeddings import LlamaCppEmbeddings\n",
    "from langchain.callbacks.base import BaseCallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_community.utilities import GoogleSearchAPIWrapper\n",
    "from langchain_core.tools import Tool\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "from langchain.agents import AgentType, Tool, initialize_agent\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = \"/mnt/c/Users/Danny/Downloads/mistral-7b-instruct-v0.2.Q5_K_M.gguf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from /mnt/c/Users/Danny/Downloads/mistral-7b-instruct-v0.2.Q5_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 17\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  23:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q5_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q5_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.78 GiB (5.67 BPW) \n",
      "llm_load_print_meta: general.name     = mistralai_mistral-7b-instruct-v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
      "llm_load_tensors:        CPU buffer size =  4892.99 MiB\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 2048\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   256.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_new_context_with_model:        CPU input buffer size   =    13.02 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   160.00 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'tokenizer.chat_template': \"{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token}}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}\", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.rope.freq_base': '1000000.000000', 'llama.context_length': '32768', 'general.name': 'mistralai_mistral-7b-instruct-v0.2', 'tokenizer.ggml.add_bos_token': 'true', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '17'}\n",
      "Guessed chat format: mistral-instruct\n"
     ]
    }
   ],
   "source": [
    "model = LlamaCpp(\n",
    "        model_path=MODEL_PATH,\n",
    "        temperature=0.75,\n",
    "        n_gpu_layers = -1,\n",
    "        n_batch = 512,\n",
    "        max_tokens=2000,\n",
    "        top_p=1,\n",
    "        callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]),\n",
    "        verbose=True,  # Verbose is required to pass to the callback manager\n",
    "        n_ctx=2048\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" V CRED SDN. BHD.   202301020466  (1514388 P)  \\n69, Jalan Setiakasih 1,  \\nBukit Damansara,  \\n50490 Kuala Lumpur.  \\nwww.v -cred.com  \\n30th January  2024 \\n \\nDanial Mirza bin Madrawi,  \\n87, Jalan Kesuma 4B/1,  \\nBandar Tasik Kesuma,  \\n43700 Beranang, Selangor . \\n \\nDear Danial , \\n \\nEMPLOYMENT OFFER AT V CRED SDN. BHD. – JUNIOR  DEVELOPER  (PRODUCT \\nDEVELOPMENT ) \\n \\nThank you for your interest in joining V Cred  Sdn. Bhd. (the “ Company ”). \\n \\nWe are pleased to offer you the position above effective from 8th April 2024 . You will be \\nreporting directly to Tan Nian Wei , Chief Technology Officer  (CTO)  of V Cred Sdn. Bhd . as \\nyour supervisor . \\n \\nThe terms of the employment are as follows :- \\n• Employment type: Full-time contract basis for 6 months . \\n \\n• Location: Flexi working  location subject to supervisor’s  discretion . \\no Main office : Cubic , PETRONAS Leadership Centre, Jalan Institusi, Kawasan \\nInstitusi Bangi, 43000 Kajang, Selangor . \\no Other location s based on work requirement s, including  PETRONAS Twin \\nTowers, Kuala Lumpur City Centre, 50088 Kuala Lumpur and other client \\noffices.  \\n \\n• Remuneration : \\no RM5,000.00  monthly basic salary , which shall be paid, net of tax and \\nstatutory contributions.  \\no Contributions will be made by the Company towards the Employees Provident \\nFund (“EPF”) in respect of your employment along with  the necessary \\ndeductions from your salary towards your portion of contributions . \\no The salary will be paid out on the 28th day of the month . \\n \\n• Laptop/Desktop: The Company operates a BYOD (Bring your own device) scheme \\nwhich means that employees are expected to bring and use their own device for work.  \\n \\n• Extension: One (1) month prior to the end of the contract period, the Company will \\nconduct a review of your performance and may make reasonable efforts to extend the \\ncontract subject to business  needs and other relevant considerations. The extension, \\nalong with its duration and any adjustments to the current terms, encompassing, but \\nnot restricted to, compensation, role, and position title, will be communicated in writing . • Termination:  \\no Either party may terminate this employment arrangement  at any time  by giving \\nthe other  written notice as follows:  \\n▪ The employee may terminate the employment by providing the \\nCompany with one (1) month's written notice of termination . \\n▪ The Company reserves the right to terminate the employment with \\none (1) week's written notice.  \\no Notwithstanding the above, this employment  may be terminated forthwith by \\nthe Company  without notice if you are guilty of inter alia misconduct, dereliction \\nof duty, or any other reason deemed reasonable at the Company’s sole \\ndiscretion.  \\no Upon termination, the salary will be pro -rated using the following calculation:  \\nDay rate = Salary/working days in a month  \\nPro-rated salary = Day rate x number of days worked in a month  \\no Upon termination, you agree not to disclose or make use of any confidential \\ninformation acquired in relation to your employment and will no longer make \\nyourself out as having continuing connection with or interest in the Company.  \\n \\n• Probation:  \\no You will undergo a one (1) month probation, during which time your \\nperformance, conduct and suitability for the role will be assessed.  \\no Your employment may be terminated during this probation period by either \\nparty at any time by giving the other written notice  as follows:  \\n▪ The employee may terminate the employment by providing the \\nCompany with one (1) week’s written notice of termination.  \\n▪ The Company reserves the right to terminate the employment with three  \\n(3) days’ written notice.  \\n \\n• Intellectual Property (IP): All IP created by the employee during the employment  \\nperiod , including inventions and software, belongs exclusively to the Company. The \\nemployee assures that no IP from previous employ ment  is brought in and agrees not \\nto take Company -developed IP upon leaving . IP created for the Company remains the \\nCompany's exclusive property  and shall not be used by the employee beyond the \\nemployment duration unless otherwise permitted by the Company . \\n \\n• Key roles:  \\no Working closely with the CTO  on the development of software applications with \\nNatural Language Processing (NLP)  capabilities for client s. \\no Design, implement, and deploy scalable models / algorithms and data pipelines \\nfor use in production.  \\no Integrate and apply Natural Language Processing (NLP) tools and Large \\nLanguage Models (LLM) into the application  as required  \\no Process and clean data from various sources including unstructured text and \\nlarge databases.  \\no Demonstrate positivity, flexibility and adaptability in handling diverse \\nresponsibilities and tasks as required by the team.  \\no Other tasks to be determined by the Company and/or the supervisor from time \\nto time . Please confirm your acceptance of this offer by signing below and returning the signed  copy \\nof this offer letter  within 7 days  from the date herein . \\n \\nWe look forward to welcoming you onboard.  \\n \\nThank you.  \\n \\n \\n \\n \\n__________________  \\n \\nLAW MAY  \\nChief Executive Officer  \\nV Cred  Sdn. Bhd . \\n  \\nRETURN RECEIPT  \\n \\nI, ___________Danial\\tMirza\\tbin\\tMadrawi_____________  (IC no: ____960512055039____), \\naccept  / decline * the employment  offer with the terms  as stipulated  in this letter .  \\n \\nUpon  acceptance , I hereby  declare  that: \\n• I will carry  out my duties  with diligence , expertise , and due care. \\n• I will abide  by the rules  and core values  of the Company . \\n• I will protect  the Intellectual  Property  (IP) of the Company  and absolve  any claims  \\nunless  otherwise  permitted  by the Company . \\n• I have  no criminal  record  or legal  charges  against  me. \\n• I have  no integrity  issue s and will declare  any conflict  of interest  with the Company  or \\nany of its clients . \\n• I have  no health  or critical  illness  issues  and am able to perform  the tasks  expected  of \\nme. \\n• I shall  not work  for, advise  or in any other  way assist  (whether  for financial  gain or \\notherwise ) any other  person , firm, business  or company  at any time whatsoever  whilst  \\nthis employment  subsists  without  prior consent  from the Company . \\n• I will use my best efforts  to promote  and protect  the interests  of the Company  and \\nobserve  the utmost  good  faith towards  the Company . \\n• The Company  has the right to mitigate  my employment , including  via warning  or \\ntermination , should  any of the above  not be met satisfactorily . \\n \\nBank  Account  Information  (for paying  salary ) \\n• Bank  account  number : _____70\\t5427331\\t0_____ \\n• Bank  name :  ________CIMB_________________ \\n \\nEmergency  Contact  \\n• Name :  ___Nur\\tIzzah\\tWong\\tbinti\\tAbdullah______ \\n• Contact  number :  ______60192411672________ \\n• Relationship :  _________Mother_____________ \\n \\nThank  you. \\n \\n \\n \\n_____________________________ \\n \\nFull Name:\\tDanial\\tMirza\\tbin\\tMadrawi  \\nDate :\\t02/02/2024  \\n \\n*Please  strikethrough  where  relevant  \\n  \\nBENEFITS SUMMAR Y \\nThe Company provides benefits for its eligible full -time contract  employees. However, the \\nCompany reserves the right to eliminate or modify any of its benefits at any time.  \\nThe contents outlined are not intended to be exhaustive. For further details , employees are \\nencouraged to refer to their direct supervisor . \\n \\nNo. Item Details  \\n1 Annual Leave * 14 days per year  \\n2 Medical  Leave * 30 days per year  \\n3 Marriage  Leave  3 days per year  \\n4 Maternity Leave  90 consecutive days  \\n5 Paternity Leave  5 days  \\n6 Compassionate Leave  2 consecutive days (appl ies to death of employee’s \\nspouse, children, sibling, parents, grandparents, \\nspouse’s parents and grandparents)  \\n7 Medical Coverage  Pay and claim  \\nMaximum RM500 per year (inclusive of spouse and \\nchildren only)  \\nDoes not cover optical and dental  expenses  \\n \\n* On pro rata basis depending on contract duration, subject to supervisor’s discretion  \\nExample: 6 months contract = 7 days annual leave, 15 days medical leave  \""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing_extensions import Concatenate\n",
    "loader = PdfReader('/mnt/c/Users/Danny/Downloads/V-Cred Offer Letter - Danial Mirza.pdf')\n",
    "raw_text = ''\n",
    "for i, page in enumerate(loader.pages):\n",
    "    content = page.extract_text()\n",
    "    if content:\n",
    "        raw_text += content\n",
    "raw_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter(\n",
    "    separator = \"\\n\",\n",
    "    chunk_size = 800,\n",
    "    chunk_overlap  = 200,\n",
    "    length_function = len,\n",
    ")\n",
    "texts = text_splitter.split_text(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from /mnt/c/Users/Danny/Downloads/mistral-7b-instruct-v0.2.Q5_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 17\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  23:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q5_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q5_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.78 GiB (5.67 BPW) \n",
      "llm_load_print_meta: general.name     = mistralai_mistral-7b-instruct-v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
      "llm_load_tensors:        CPU buffer size =  4892.99 MiB\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =    64.00 MiB\n",
      "llama_new_context_with_model: KV self size  =   64.00 MiB, K (f16):   32.00 MiB, V (f16):   32.00 MiB\n",
      "llama_new_context_with_model:        CPU input buffer size   =     0.14 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =     1.13 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'tokenizer.chat_template': \"{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token}}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}\", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.rope.freq_base': '1000000.000000', 'llama.context_length': '32768', 'general.name': 'mistralai_mistral-7b-instruct-v0.2', 'tokenizer.ggml.add_bos_token': 'true', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '17'}\n",
      "Guessed chat format: mistral-instruct\n",
      "\n",
      "llama_print_timings:        load time =    1021.87 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =    1020.79 ms /     8 tokens (  127.60 ms per token,     7.84 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    1021.94 ms /     9 tokens\n",
      "\n",
      "llama_print_timings:        load time =    1021.87 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =     890.84 ms /     8 tokens (  111.35 ms per token,     8.98 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     892.05 ms /     9 tokens\n",
      "\n",
      "llama_print_timings:        load time =    1021.87 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =     891.47 ms /     8 tokens (  111.43 ms per token,     8.97 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     893.11 ms /     9 tokens\n",
      "\n",
      "llama_print_timings:        load time =    1021.87 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =     885.38 ms /     8 tokens (  110.67 ms per token,     9.04 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     886.81 ms /     9 tokens\n",
      "\n",
      "llama_print_timings:        load time =    1021.87 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =     883.66 ms /     8 tokens (  110.46 ms per token,     9.05 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     884.87 ms /     9 tokens\n",
      "\n",
      "llama_print_timings:        load time =    1021.87 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =     878.26 ms /     8 tokens (  109.78 ms per token,     9.11 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     879.77 ms /     9 tokens\n",
      "\n",
      "llama_print_timings:        load time =    1021.87 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =     830.89 ms /     8 tokens (  103.86 ms per token,     9.63 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     832.62 ms /     9 tokens\n",
      "\n",
      "llama_print_timings:        load time =    1021.87 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =     866.63 ms /     8 tokens (  108.33 ms per token,     9.23 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     868.27 ms /     9 tokens\n",
      "\n",
      "llama_print_timings:        load time =    1021.87 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =     894.28 ms /     8 tokens (  111.79 ms per token,     8.95 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     895.68 ms /     9 tokens\n",
      "\n",
      "llama_print_timings:        load time =    1021.87 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =     877.81 ms /     8 tokens (  109.73 ms per token,     9.11 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     879.05 ms /     9 tokens\n",
      "\n",
      "llama_print_timings:        load time =    1021.87 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =     878.72 ms /     8 tokens (  109.84 ms per token,     9.10 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     880.40 ms /     9 tokens\n",
      "\n",
      "llama_print_timings:        load time =    1021.87 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =     886.81 ms /     8 tokens (  110.85 ms per token,     9.02 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     888.83 ms /     9 tokens\n",
      "\n",
      "llama_print_timings:        load time =    1021.87 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =     833.67 ms /     8 tokens (  104.21 ms per token,     9.60 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     835.01 ms /     9 tokens\n",
      "\n",
      "llama_print_timings:        load time =    1021.87 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =     889.64 ms /     8 tokens (  111.21 ms per token,     8.99 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     890.65 ms /     9 tokens\n"
     ]
    }
   ],
   "source": [
    "document_search = Chroma.from_texts(texts, LlamaCppEmbeddings(model_path=MODEL_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.question_answering import load_qa_chain\n",
    "chain = load_qa_chain(llm=model, chain_type=\"stuff\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    1021.87 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =    1121.39 ms /     8 tokens (  140.17 ms per token,     7.13 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    1122.52 ms /     9 tokens\n",
      "/home/danialmirxa/miniforge-pypy3/envs/mixtral/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `run` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Based on the information provided in the employment agreement, if you resign after two weeks, your employment may be terminated without notice by the Company. Additionally, you may not be entitled to any severance pay or other benefits. It is recommended that you carefully review your employment agreement and consult with a legal professional if you have any questions or concerns regarding your resignation or the potential consequences thereof."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   41865.54 ms\n",
      "llama_print_timings:      sample time =      13.42 ms /    80 runs   (    0.17 ms per token,  5961.70 tokens per second)\n",
      "llama_print_timings: prompt eval time =   77729.81 ms /   953 tokens (   81.56 ms per token,    12.26 tokens per second)\n",
      "llama_print_timings:        eval time =   22497.40 ms /    79 runs   (  284.78 ms per token,     3.51 tokens per second)\n",
      "llama_print_timings:       total time =  100571.38 ms /  1032 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Based on the information provided in the employment agreement, if you resign after two weeks, your employment may be terminated without notice by the Company. Additionally, you may not be entitled to any severance pay or other benefits. It is recommended that you carefully review your employment agreement and consult with a legal professional if you have any questions or concerns regarding your resignation or the potential consequences thereof.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What will happen if resign after 2 weeks?\"\n",
    "docs = document_search.similarity_search(query)\n",
    "chain.run(input_documents=docs, question=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "search = GoogleSearchAPIWrapper()\n",
    "\n",
    "\n",
    "def top5_results(query):\n",
    "    return search.results(query, 5)\n",
    "\n",
    "\n",
    "tools = Tool(\n",
    "    name=\"Google Search Snippets\",\n",
    "    description=\"Search Google for recent results.\",\n",
    "    func=top5_results,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': \"Barack Obama's birth certificate\",\n",
       " 'link': 'https://obamawhitehouse.archives.gov/sites/default/files/rss_viewer/birth-certificate-long-form.pdf',\n",
       " 'snippet': 'Full Maiden Name of Mother. STANLEY. 15. Age of Mother 16. 18. Yes. No. 9. Race of Father. OBAMA. African. Usual Occupation. 12b. Student. DUNHAM. Kind of\\xa0...'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools.run(\"Obama's first name?\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory(return_messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"• Termination:  \\no Either party may terminate this employment arrangement  at any time  by giving \\nthe other  written notice as follows:  \\n▪ The employee may terminate the employment by providing the \\nCompany with one (1) month's written notice of termination . \\n▪ The Company reserves the right to terminate the employment with \\none (1) week's written notice.  \\no Notwithstanding the above, this employment  may be terminated forthwith by \\nthe Company  without notice if you are guilty of inter alia misconduct, dereliction \\nof duty, or any other reason deemed reasonable at the Company’s sole \\ndiscretion.  \\no Upon termination, the salary will be pro -rated using the following calculation:  \\nDay rate = Salary/working days in a month  \\nPro-rated salary = Day rate x number of days worked in a month  \\no Upon termination, you agree not to disclose or make use of any confidential \\ninformation acquired in relation to your employment and will no longer make \\nyourself out as having continuing connection with or interest in the Company.  \\n \\n• Probation:  \\no You will undergo a one (1) month probation, during which time your \\nperformance, conduct and suitability for the role will be assessed.  \\no Your employment may be terminated during this probation period by either \\nparty at any time by giving the other written notice  as follows:  \\n▪ The employee may terminate the employment by providing the \\nCompany with one (1) week’s written notice of termination.  \\n▪ The Company reserves the right to terminate the employment with three  \\n(3) days’ written notice.  \\n \\n• Intellectual Property (IP): All IP created by the employee during the employment  \\nperiod , including inventions and software, belongs exclusively to the Company. The \\nemployee assures that no IP from previous employ ment  is brought in and agrees not \\nto take Company -developed IP upon leaving . IP created for the Company remains the \\nCompany's exclusive property  and shall not be used by the employee beyond the \\nemployment duration unless otherwise permitted by the Company . \\n \\n• Key roles:  \\no Working closely with the CTO  on the development of software applications with \\nNatural Language Processing (NLP)  capabilities for client s. \\no Design, implement, and deploy scalable models / algorithms and data pipelines \\nfor use in production.  \\no Integrate and apply Natural Language Processing (NLP) tools and Large \\nLanguage Models (LLM) into the application  as required  \\no Process and clean data from various sources including unstructured text and \\nlarge databases.  \\no Demonstrate positivity, flexibility and adaptability in handling diverse \\nresponsibilities and tasks as required by the team.  \\no Other tasks to be determined by the Company and/or the supervisor from time \\nto time .\""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Internet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_result = RetrievalQA.from_chain_type(\n",
    "    llm=model, chain_type=\"stuff\", retriever=document_search.as_retriever()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [Tool(\n",
    "    name=\"Google Search Snippets\",\n",
    "    description=\"Search Google for recent results.\",\n",
    "    func=top5_results,\n",
    ")]\n",
    "tools += [\n",
    "    Tool(\n",
    "        name=\"Google Search Result\",\n",
    "        func=search_result.run,\n",
    "        description=\"useful for when you need to answer questions about the most recent state of the union address. Input should be a fully formed question.\",\n",
    "    ),   \n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/danialmirxa/miniforge-pypy3/envs/mixtral/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `initialize_agent` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use Use new agent constructor methods like create_react_agent, create_json_agent, create_structured_chat_agent, etc. instead.\n",
      "  warn_deprecated(\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      " I need to find out what V-Cred is\n",
      "Action: Google Search Result\n",
      "Action Input: V-Cred definition"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   41865.54 ms\n",
      "llama_print_timings:      sample time =       4.78 ms /    28 runs   (    0.17 ms per token,  5856.52 tokens per second)\n",
      "llama_print_timings: prompt eval time =   19320.92 ms /   190 tokens (  101.69 ms per token,     9.83 tokens per second)\n",
      "llama_print_timings:        eval time =    7390.72 ms /    27 runs   (  273.73 ms per token,     3.65 tokens per second)\n",
      "llama_print_timings:       total time =   26815.06 ms /   217 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m I need to find out what V-Cred is\n",
      "Action: Google Search Result\n",
      "Action Input: V-Cred definition\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    1021.87 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =     551.26 ms /     5 tokens (  110.25 ms per token,     9.07 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     551.89 ms /     6 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " V-Cred is a term used in the field of Natural Language Processing (NLP) and Machine Learning (ML) for credential verification. V-Cred refers to a system or model designed to analyze and validate various types of credentials such as degrees, certificates, licenses, and professional memberships. The V-Cred system uses advanced NLP and ML techniques such as named entity recognition (NER), part-of-speech (POS) tagging, and deep learning models to extract relevant information from credential documents and databases. The V-Cred system also uses various data sources such as government databases, educational institutions, and professional organizations to cross-reference and validate credential information. The V-Cred system provides credential verification services to various industries such as education, healthcare, finance, and government. The V-Cred system helps organizations to streamline their credential verification processes, reduce fraud and errors, improve compliance with regulatory requirements, and enhance the overall security and integrity of their operations."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   41865.54 ms\n",
      "llama_print_timings:      sample time =      40.00 ms /   205 runs   (    0.20 ms per token,  5125.13 tokens per second)\n",
      "llama_print_timings: prompt eval time =   69986.25 ms /   877 tokens (   79.80 ms per token,    12.53 tokens per second)\n",
      "llama_print_timings:        eval time =   72049.00 ms /   204 runs   (  353.18 ms per token,     2.83 tokens per second)\n",
      "llama_print_timings:       total time =  142955.08 ms /  1081 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Observation: \u001b[33;1m\u001b[1;3m V-Cred is a term used in the field of Natural Language Processing (NLP) and Machine Learning (ML) for credential verification. V-Cred refers to a system or model designed to analyze and validate various types of credentials such as degrees, certificates, licenses, and professional memberships. The V-Cred system uses advanced NLP and ML techniques such as named entity recognition (NER), part-of-speech (POS) tagging, and deep learning models to extract relevant information from credential documents and databases. The V-Cred system also uses various data sources such as government databases, educational institutions, and professional organizations to cross-reference and validate credential information. The V-Cred system provides credential verification services to various industries such as education, healthcare, finance, and government. The V-Cred system helps organizations to streamline their credential verification processes, reduce fraud and errors, improve compliance with regulatory requirements, and enhance the overall security and integrity of their operations.\u001b[0m\n",
      "Thought: I now know what V-Cred is and what it does\n",
      "Final Answer: V-Cred is a term used in the field of Natural Language Processing (NLP) and Machine Learning (ML) for credential verification. It refers to a system or model designed to analyze and validate various types of credentials such as degrees, certificates, licenses, and professional memberships. The V-Cred system uses advanced NLP and ML techniques such as named entity recognition (NER), part-of-speech (POS) tagging, and deep learning models to extract relevant information from credential documents and databases. The V-Cred system also uses various data sources such as government databases, educational institutions, and professional organizations to cross-reference and validate credential information. The V-Cred system provides credential verification services to various industries such as education, healthcare, finance, and government. It helps organizations to streamline their credential verification processes, reduce fraud and errors, improve compliance with regulatory requirements, and enhance the overall security and integrity of their operations."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   41865.54 ms\n",
      "llama_print_timings:      sample time =      39.52 ms /   215 runs   (    0.18 ms per token,  5440.97 tokens per second)\n",
      "llama_print_timings: prompt eval time =   33160.62 ms /   427 tokens (   77.66 ms per token,    12.88 tokens per second)\n",
      "llama_print_timings:        eval time =   63086.84 ms /   214 runs   (  294.80 ms per token,     3.39 tokens per second)\n",
      "llama_print_timings:       total time =   97136.97 ms /   641 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m I now know what V-Cred is and what it does\n",
      "Final Answer: V-Cred is a term used in the field of Natural Language Processing (NLP) and Machine Learning (ML) for credential verification. It refers to a system or model designed to analyze and validate various types of credentials such as degrees, certificates, licenses, and professional memberships. The V-Cred system uses advanced NLP and ML techniques such as named entity recognition (NER), part-of-speech (POS) tagging, and deep learning models to extract relevant information from credential documents and databases. The V-Cred system also uses various data sources such as government databases, educational institutions, and professional organizations to cross-reference and validate credential information. The V-Cred system provides credential verification services to various industries such as education, healthcare, finance, and government. It helps organizations to streamline their credential verification processes, reduce fraud and errors, improve compliance with regulatory requirements, and enhance the overall security and integrity of their operations.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'V-Cred is a term used in the field of Natural Language Processing (NLP) and Machine Learning (ML) for credential verification. It refers to a system or model designed to analyze and validate various types of credentials such as degrees, certificates, licenses, and professional memberships. The V-Cred system uses advanced NLP and ML techniques such as named entity recognition (NER), part-of-speech (POS) tagging, and deep learning models to extract relevant information from credential documents and databases. The V-Cred system also uses various data sources such as government databases, educational institutions, and professional organizations to cross-reference and validate credential information. The V-Cred system provides credential verification services to various industries such as education, healthcare, finance, and government. It helps organizations to streamline their credential verification processes, reduce fraud and errors, improve compliance with regulatory requirements, and enhance the overall security and integrity of their operations.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent = initialize_agent(\n",
    "    tools, model, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True\n",
    ")\n",
    "agent.run(\n",
    "    \"What is V-Cred?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mixtral",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
